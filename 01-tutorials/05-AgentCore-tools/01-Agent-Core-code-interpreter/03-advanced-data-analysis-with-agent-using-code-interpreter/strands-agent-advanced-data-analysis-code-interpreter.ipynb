{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "746626f6b18e1c8c",
   "metadata": {},
   "source": [
    "## Advanced Data Analysis using Amazon AgentCore Bedrock Code Interpreter- Tutorial(Strands)\n",
    "This tutorial demonstrates how to create an AI agent that performs advanced data analysis through code execution using Python. We use Amazon Bedrock AgentCore Code Interpreter to run code that is generated by the LLM.\n",
    "\n",
    "This tutorial demonstrates how to use AgentCore Bedrock Code Interpreter to:\n",
    "1. Set up a sandbox environment\n",
    "2. Configure a strands based agent that performs advanced data analysis by generating code based on the user query\n",
    "3. Execute code in a sandbox environment using Code Interpreter\n",
    "4. Display the results back to the user\n",
    "\n",
    "## Prerequisites\n",
    "- AWS account with Bedrock AgentCore Code Interpreter access\n",
    "- You have the necessary IAM permissions to create and manage code interpreter resources\n",
    "- Required Python packages installed(including boto3, bedrock-agentcore & strands)\n",
    "- IAM role should have permissions to invoke models on Amazon Bedrock\n",
    " - Access to Claude 3.7 Sonnet model in the US Oregon (us-west-2) region (default model for Strands SDK)\n",
    "\n",
    "## Your IAM execution role should have the following IAM policy attached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f323388415caf3f7",
   "metadata": {},
   "source": [
    "~~~ {\n",
    "\"Version\": \"2012-10-17\",\n",
    "\"Statement\": [\n",
    "    {\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Action\": [\n",
    "            \"bedrock-agentcore:CreateCodeInterpreter\",\n",
    "            \"bedrock-agentcore:StartCodeInterpreterSession\",\n",
    "            \"bedrock-agentcore:InvokeCodeInterpreter\",\n",
    "            \"bedrock-agentcore:StopCodeInterpreterSession\",\n",
    "            \"bedrock-agentcore:DeleteCodeInterpreter\",\n",
    "            \"bedrock-agentcore:ListCodeInterpreters\",\n",
    "            \"bedrock-agentcore:GetCodeInterpreter\"\n",
    "        ],\n",
    "        \"Resource\": \"*\"\n",
    "    },\n",
    "    {\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Action\": [\n",
    "            \"logs:CreateLogGroup\",\n",
    "            \"logs:CreateLogStream\",\n",
    "            \"logs:PutLogEvents\"\n",
    "        ],\n",
    "        \"Resource\": \"arn:aws:logs:*:*:log-group:/aws/bedrock-agentcore/code-interpreter*\"\n",
    "    }\n",
    "]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b2cb86ff18d9c",
   "metadata": {},
   "source": [
    "## How it works\n",
    "\n",
    "The code execution sandbox enables agents to safely process user queries by creating an isolated environment with a code interpreter, shell, and file system. After a Large Language Model helps with tool selection, code is executed within this session, before being returned to the user or Agent for synthesis.\n",
    "\n",
    "![architecture local](code-interpreter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859482709c77b03d",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "\n",
    "First, let's import the necessary libraries and initialize our Code Interpreter client.\n",
    "\n",
    "The default session timeout is 900 seconds(15 minutes). However, we start the session with a slightly session timeout duration of 1200 seconds(20 minutes), since we will perform detailed analysis on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13da423bac8ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bb006310a96750c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:48.346322Z",
     "start_time": "2025-07-13T09:35:46.244470Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01K01J7Z3DH445CJ0YW14BK2GY'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bedrock_agentcore.tools.code_interpreter_client import CodeInterpreter\n",
    "from strands import Agent, tool\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Initialize the Code Interpreter within a supported AWS region.\n",
    "code_client = CodeInterpreter('us-west-2')\n",
    "code_client.start(session_timeout_seconds=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd02b57bd10dda2",
   "metadata": {},
   "source": [
    "## 2. Reading Local Data File\n",
    "\n",
    "Now we'll read the contents of our sample data file. The file consists of random data with 4 columns: Name, Preferred_City, Preferred_Animal, Preferred_Thing and ~ 300,000 records.\n",
    "\n",
    "We will analyze this file using an agent little later, to understand distributions and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bef3821f290a589b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:48.465278Z",
     "start_time": "2025-07-13T09:35:48.385287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Preferred_City</th>\n",
       "      <th>Preferred_Animal</th>\n",
       "      <th>Preferred_Thing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Betty Ramirez</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>Elephant</td>\n",
       "      <td>Sofa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jennifer Green</td>\n",
       "      <td>Naples</td>\n",
       "      <td>Bee</td>\n",
       "      <td>Shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Lopez</td>\n",
       "      <td>Helsinki</td>\n",
       "      <td>Zebra</td>\n",
       "      <td>Wallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Susan Gonzalez</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>Chicken</td>\n",
       "      <td>Phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jennifer Wright</td>\n",
       "      <td>Buenos Aires</td>\n",
       "      <td>Goat</td>\n",
       "      <td>Wallet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name Preferred_City Preferred_Animal Preferred_Thing\n",
       "0    Betty Ramirez         Dallas         Elephant            Sofa\n",
       "1   Jennifer Green         Naples              Bee           Shirt\n",
       "2       John Lopez       Helsinki            Zebra          Wallet\n",
       "3   Susan Gonzalez        Beijing          Chicken           Phone\n",
       "4  Jennifer Wright   Buenos Aires             Goat          Wallet"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv(\"samples/data.csv\")\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38277480cbc38ba0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:48.503763Z",
     "start_time": "2025-07-13T09:35:48.495825Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(file_path: str) -> str:\n",
    "    \"\"\"Helper function to read file content with error handling\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' was not found.\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "data_file_content = read_file(\"samples/data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc91e0fb83fa05",
   "metadata": {},
   "source": [
    "## 3. Preparing Files for Sandbox Environment\n",
    "\n",
    "We'll create a structure that defines the files we want to create in the sandbox environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "da44fb745b84c6ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:49.703849Z",
     "start_time": "2025-07-13T09:35:49.699079Z"
    }
   },
   "outputs": [],
   "source": [
    "files_to_create = [\n",
    "                {\n",
    "                    \"path\": \"data.csv\",\n",
    "                    \"text\": data_file_content\n",
    "                }]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f055bea34c93279",
   "metadata": {},
   "source": [
    "## 4. Creating Helper Function for Tool Invocation\n",
    "\n",
    "This helper function will make it easier to call sandbox tools and handle their responses. Within an active session, you can execute code in supported languages (Python, JavaScript), access libraries based on your dependencies configuration, generate visualizations, and maintain state between executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a74164c54b3b8ad5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:50.359366Z",
     "start_time": "2025-07-13T09:35:50.356755Z"
    }
   },
   "outputs": [],
   "source": [
    "def call_tool(tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Helper function to invoke sandbox tools\n",
    "\n",
    "    Args:\n",
    "        tool_name (str): Name of the tool to invoke\n",
    "        arguments (Dict[str, Any]): Arguments to pass to the tool\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: JSON formatted result\n",
    "    \"\"\"\n",
    "    response = code_client.invoke(tool_name, arguments)\n",
    "    for event in response[\"stream\"]:\n",
    "        return json.dumps(event[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd33790785ac084a",
   "metadata": {},
   "source": [
    "## 5. Write data file to Code Sandbox\n",
    "\n",
    "Now we'll write our data file into the sandbox environment and verify they were created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "380afae3a5ba4934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:36:00.346136Z",
     "start_time": "2025-07-13T09:35:50.965773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing files result:\n",
      "{\"content\": [{\"type\": \"text\", \"text\": \"Successfully wrote all 1 files\"}], \"isError\": false}\n",
      "\n",
      "Files in sandbox:\n",
      "{\"content\": [{\"type\": \"resource_link\", \"uri\": \"file:///log\", \"name\": \"log\", \"description\": \"Directory\"}, {\"type\": \"resource_link\", \"mimeType\": \"text/csv\", \"uri\": \"file:///data.csv\", \"name\": \"data.csv\", \"description\": \"File\"}, {\"type\": \"resource_link\", \"uri\": \"file:///.ipython\", \"name\": \".ipython\", \"description\": \"Directory\"}], \"isError\": false}\n"
     ]
    }
   ],
   "source": [
    "# Write files to sandbox\n",
    "writing_files = call_tool(\"writeFiles\", {\"content\": files_to_create})\n",
    "print(\"Writing files result:\")\n",
    "print(writing_files)\n",
    "\n",
    "# Verify files were created\n",
    "listing_files = call_tool(\"listFiles\", {\"path\": \"\"})\n",
    "print(\"\\nFiles in sandbox:\")\n",
    "print(listing_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640eae7a52ce9f8d",
   "metadata": {},
   "source": [
    "## 6. Perform Advanced Analysis using Strands based Agent\n",
    "\n",
    "Now we will configure an agent to perform data analysis on the data file that we uploaded into the sandbox(above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3b068faf4a5eaa",
   "metadata": {},
   "source": [
    "### 6.1 System Prompt Definition\n",
    "Define the behavior and capabilities of the AI assistant. We instruct our assistant to always validate answers through code execution and data based reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6e6830a170b45ce3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:36:00.366216Z",
     "start_time": "2025-07-13T09:36:00.364374Z"
    }
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant that validates all answers through code execution using the tools provided. DO NOT Answer questions without using the tools\n",
    "\n",
    "VALIDATION PRINCIPLES:\n",
    "1. When making claims about code, algorithms, or calculations - write code to verify them\n",
    "2. Use execute_python to test mathematical calculations, algorithms, and logic\n",
    "3. Create test scripts to validate your understanding before giving answers\n",
    "4. Always show your work with actual code execution\n",
    "5. If uncertain, explicitly state limitations and validate what you can\n",
    "\n",
    "APPROACH:\n",
    "- If asked about a programming concept, implement it in code to demonstrate\n",
    "- If asked for calculations, compute them programmatically AND show the code\n",
    "- If implementing algorithms, include test cases to prove correctness\n",
    "- Document your validation process for transparency\n",
    "- The sandbox maintains state between executions, so you can refer to previous results\n",
    "\n",
    "TOOL AVAILABLE:\n",
    "- execute_python: Run Python code and see output\n",
    "\n",
    "RESPONSE FORMAT: The execute_python tool returns a JSON response with:\n",
    "- sessionId: The sandbox session ID\n",
    "- id: Request ID\n",
    "- isError: Boolean indicating if there was an error\n",
    "- content: Array of content objects with type and text/data\n",
    "- structuredContent: For code execution, includes stdout, stderr, exitCode, executionTime\n",
    "\n",
    "For successful code execution, the output will be in content[0].text and also in structuredContent.stdout.\n",
    "Check isError field to see if there was an error.\n",
    "\n",
    "Be thorough, accurate, and always validate your answers when possible.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87157a0ea835ab5",
   "metadata": {},
   "source": [
    "### 6.2 Code Execution Tool Definition\n",
    "Next we define the function as tool that will be used by the Agent as tool, to run code in the code sandbox. We use the @tool decorator to annotate the function as a custom tool for the Agent.\n",
    "\n",
    "Within an active code interpreter session, you can execute code in supported languages (Python, JavaScript), access libraries based on your dependencies configuration, generate visualizations, and maintain state between executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "750472cd96e873c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:36:34.464620Z",
     "start_time": "2025-07-13T09:36:34.457484Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define and configure the code interpreter tool\n",
    "@tool\n",
    "def execute_python(code: str, description: str = \"\") -> str:\n",
    "    \"\"\"Execute Python code in the sandbox.\"\"\"\n",
    "\n",
    "    if description:\n",
    "        code = f\"# {description}\\n{code}\"\n",
    "\n",
    "    #Print generated Code to be executed\n",
    "    print(f\"\\n Generated Code: {code}\")\n",
    "\n",
    "\n",
    "    # Call the Invoke method and execute the generated code, within the initialized code interpreter session\n",
    "    response = code_client.invoke(\"executeCode\", {\n",
    "        \"code\": code,\n",
    "        \"language\": \"python\",\n",
    "        \"clearContext\": False\n",
    "    })\n",
    "    for event in response[\"stream\"]:\n",
    "        return json.dumps(event[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bc47b82755730d",
   "metadata": {},
   "source": [
    "### 6.3 Agent Configuration\n",
    "We create and configure an agent using the Strands SDK. We provide it the system prompt and the tool we defined above to execute generate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "14c5e8f18b70dc01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:36:35.341080Z",
     "start_time": "2025-07-13T09:36:35.239620Z"
    }
   },
   "outputs": [],
   "source": [
    "#configure the strands agent including the tool(s)\n",
    "agent=Agent(\n",
    "        tools=[execute_python],\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        callback_handler=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25693e10aa1e5689",
   "metadata": {},
   "source": [
    "## 7. Agent Invocation and Response Processing\n",
    "We invoke the agent with our query and process the agent's response\n",
    "\n",
    "\n",
    "Note: Async execution requires running in an async environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddfa7cb97ead950",
   "metadata": {},
   "source": [
    "## 7.1 Query to perform Exploratory Data Analysis(EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f98916e2cc3627",
   "metadata": {},
   "source": [
    "Let's start with a query which instructs the agent to perform exploratory data analysis on the data file in the code sandbox environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7370ff964d06a1cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:40:07.612284Z",
     "start_time": "2025-07-13T09:37:28.402310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll perform an exploratory data analysis (EDA) on the data.csv file to examine distributions and identify any outliers. Let me work through this step by step.I see we're missing some libraries. Let me try without seaborn and use the standard libraries:Now let's analyze each categorical column in more detail:Let's check for patterns in preferences and analyze distributions visually:Let's check for outliers and get more detailed statistics:Let's further explore interesting patterns and relationships in the data:Based on the comprehensive exploratory data analysis (EDA) of the data.csv file, here's a summary of the findings:\n",
      "\n",
      "## 1. Dataset Overview\n",
      "\n",
      "- **Size**: The dataset contains 299,130 records with 4 columns: Name, Preferred_City, Preferred_Animal, and Preferred_Thing.\n",
      "- **Data types**: All columns contain categorical (object) data.\n",
      "- **Unique values**:\n",
      "  - 1,722 unique names (combinations of first and last names)\n",
      "  - 55 unique cities\n",
      "  - 50 unique animals\n",
      "  - 51 unique things\n",
      "\n",
      "## 2. Distributions\n",
      "\n",
      "### Names\n",
      "- There are 42 unique first names and 41 unique last names in the dataset.\n",
      "- First name distribution:\n",
      "  - Mean: 7,122.14 occurrences per name\n",
      "  - Most common: \"Dorothy\" (7,264 occurrences)\n",
      "  - Least common: \"Donna\" (6,952 occurrences)\n",
      "  - Standard deviation: 74.69\n",
      "\n",
      "### Cities\n",
      "- City distribution:\n",
      "  - Mean: 5,438.73 occurrences per city\n",
      "  - Most common: \"Prague\" (5,587 occurrences)\n",
      "  - Least common: \"Phoenix\" (5,236 occurrences)\n",
      "  - Standard deviation: 86.88 (1.60% of the mean)\n",
      "\n",
      "### Animals\n",
      "- Animal distribution:\n",
      "  - Mean: 5,982.60 occurrences per animal\n",
      "  - Most common: \"Goat\" (6,141 occurrences)\n",
      "  - Least common: \"Shark\" (5,761 occurrences)\n",
      "  - Standard deviation: 86.54 (1.45% of the mean)\n",
      "\n",
      "### Things\n",
      "- Thing distribution:\n",
      "  - Mean: 5,865.29 occurrences per thing\n",
      "  - Most common: \"Pencil\" (6,058 occurrences)\n",
      "  - Least common: \"Candle\" (5,720 occurrences)\n",
      "  - Standard deviation: 76.11 (1.30% of the mean)\n",
      "\n",
      "## 3. Outliers\n",
      "\n",
      "- Using the Z-score method (threshold = 3), no outliers were detected in any category.\n",
      "- Using the IQR method:\n",
      "  - Cities: No outliers detected\n",
      "  - Animals: \"Shark\" is identified as an outlier with 5,761 occurrences (below the lower bound of 5,779.25)\n",
      "  - Things: No outliers detected\n",
      "\n",
      "## 4. Patterns and Relationships\n",
      "\n",
      "### Name Duplication\n",
      "- All names appear multiple times in the dataset\n",
      "- The most common name is \"Lisa White\" which appears 222 times\n",
      "- People with the same name have different preferences\n",
      "\n",
      "### Preference Distributions\n",
      "- The distributions of preferences for cities, animals, and things are remarkably uniform\n",
      "- The small standard deviations relative to means (1.3%-1.6%) indicate very even distributions\n",
      "- This suggests the data might be synthetically generated with minor random variations\n",
      "\n",
      "### Independence Analysis\n",
      "- First names and city preferences show weak association:\n",
      "  - Each first name's most preferred city represents only 2.07%-2.36% of people with that name\n",
      "  - E.g., \"Dorothy\" prefers \"Florence\" but only 2.26% of Dorothys chose Florence\n",
      "\n",
      "### Common Combinations\n",
      "- Most common city-animal combination: Amsterdam-Pig (151 occurrences)\n",
      "- Most common animal-thing combination: Cat-Basket (151 occurrences)\n",
      "\n",
      "### Gender Analysis (Inferred from Names)\n",
      "- Slight differences in preferences between inferred genders:\n",
      "  - Female: prefer Mumbai, Atlanta, Rome cities; Tiger, Owl, Pig animals; Can, Game, Knife things\n",
      "  - Male: prefer Athens, Seoul, Venice cities; Penguin, Whale, Guinea Pig animals; Bowl, Box, Basket things\n",
      "\n",
      "## 5. Key Insights\n",
      "\n",
      "1. **Highly Uniform Data**: The remarkably even distribution across all categories suggests this might be synthetic data generated with slight randomization.\n",
      "\n",
      "2. **Weak Associations**: There appears to be little meaningful association between name and preferences, with the top preferences for each name occurring at frequencies close to what random chance would predict.\n",
      "\n",
      "3. **Minimal Outliers**: Only one outlier was detected (Shark as a preferred animal) using the IQR method, and none using Z-score, further suggesting careful data generation.\n",
      "\n",
      "4. **Name Duplication**: Each full name appears multiple times but with different preferences, indicating the dataset doesn't use name as a unique identifier.\n",
      "\n",
      "The data appears to be synthetically generated with preferences assigned in a mostly random but controlled manner to create a very uniform distribution with slight variations. The small percentage differences in preference associations are likely due to random variations rather than meaningful patterns."
     ]
    }
   ],
   "source": [
    "query = \"Perform exploratory data analysis(EDA) on the file 'data.csv'. Tell me about distributions and outlier values.\"\n",
    "\n",
    "# Invoke the agent asynchcronously and stream the response\n",
    "response_text = \"\"\n",
    "async for event in agent.stream_async(query):\n",
    "    if \"data\" in event:\n",
    "        # Stream text response\n",
    "        chunk = event[\"data\"]\n",
    "        response_text += chunk\n",
    "        print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accab0cfe53ade15",
   "metadata": {},
   "source": [
    "## 7.2 Query to extract information\n",
    "\n",
    "Now, let's instruct the agent to extract specific information from the data file in the code sandbox environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f091d1f87558bd41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:37:07.283968Z",
     "start_time": "2025-07-13T09:36:45.865171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll help you analyze the data.csv file to find individuals named 'Kimberly' who have 'Crocodile' as their favorite animal. Let me use Python to check this data file.\n",
      " Generated Code: # Checking if data.csv exists and examining its structure\n",
      "# First, let's check if the file exists\n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "# Check if the file exists\n",
      "if os.path.exists('data.csv'):\n",
      "    print(f\"File 'data.csv' exists. Reading the file...\")\n",
      "    # Try to read the CSV file\n",
      "    try:\n",
      "        df = pd.read_csv('data.csv')\n",
      "        print(\"File successfully loaded. Here's a preview:\")\n",
      "        print(df.head())\n",
      "        print(f\"\\nTotal rows in the file: {len(df)}\")\n",
      "        print(f\"Columns in the file: {df.columns.tolist()}\")\n",
      "    except Exception as e:\n",
      "        print(f\"Error reading the file: {e}\")\n",
      "else:\n",
      "    print(f\"File 'data.csv' does not exist in the current directory.\")\n",
      "    print(f\"Current directory contents: {os.listdir()}\")\n",
      "Great! The data.csv file exists and has been successfully loaded. Now I can see it has 299,130 rows and includes the columns: 'Name', 'Preferred_City', 'Preferred_Animal', and 'Preferred_Thing'.\n",
      "\n",
      "Let me now analyze the data to find individuals with the first name 'Kimberly' who have 'Crocodile' as their favorite animal:\n",
      " Generated Code: # Counting individuals named Kimberly with Crocodile as favorite animal\n",
      "import pandas as pd\n",
      "\n",
      "# Read the data\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Extract first name from the full name\n",
      "df['First_Name'] = df['Name'].str.split().str[0]\n",
      "\n",
      "# Filter for individuals with first name 'Kimberly' and 'Crocodile' as favorite animal\n",
      "filtered_df = df[(df['First_Name'] == 'Kimberly') & (df['Preferred_Animal'] == 'Crocodile')]\n",
      "\n",
      "# Get the count and display results\n",
      "count = len(filtered_df)\n",
      "print(f\"Number of individuals named Kimberly with Crocodile as favorite animal: {count}\")\n",
      "\n",
      "# If there are any matches, show a few examples\n",
      "if count > 0:\n",
      "    print(\"\\nExample records:\")\n",
      "    print(filtered_df.head(min(5, count)))\n",
      "Based on the analysis of the data.csv file, there are **120 individuals** with the first name 'Kimberly' who have 'Crocodile' as their favorite animal.\n",
      "\n",
      "The code extracted the first name from the 'Name' column and then filtered the data to find records where:\n",
      "1. The first name is 'Kimberly'\n",
      "2. The preferred animal is 'Crocodile'\n",
      "\n",
      "The analysis shows examples of these individuals, including their full names, preferred cities, and preferred things. This confirms the answer of 120 individuals.\n",
      " Generated Code: # Basic information about the dataset\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Load the dataset\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Basic information\n",
      "print(\"Dataset Shape:\", df.shape)\n",
      "print(\"\\nData Types:\")\n",
      "print(df.dtypes)\n",
      "print(\"\\nFirst 5 rows:\")\n",
      "print(df.head())\n",
      "print(\"\\nSummary Statistics:\")\n",
      "print(df.describe(include='all'))\n",
      "\n",
      " Generated Code: # Basic information about the dataset\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Load the dataset\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Basic information\n",
      "print(\"Dataset Shape:\", df.shape)\n",
      "print(\"\\nData Types:\")\n",
      "print(df.dtypes)\n",
      "print(\"\\nFirst 5 rows:\")\n",
      "print(df.head())\n",
      "print(\"\\nSummary Statistics:\")\n",
      "print(df.describe(include='all'))\n",
      "\n",
      " Generated Code: # Analyzing distributions of categorical variables\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Load the dataset\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Extract first name from the full name\n",
      "df['First_Name'] = df['Name'].str.split().str[0]\n",
      "df['Last_Name'] = df['Name'].str.split().str[1]\n",
      "\n",
      "# Display most common first names\n",
      "print(\"Top 10 Most Common First Names:\")\n",
      "print(df['First_Name'].value_counts().head(10))\n",
      "print(\"\\nLeast Common First Names:\")\n",
      "print(df['First_Name'].value_counts().tail(5))\n",
      "\n",
      "# Display most common last names\n",
      "print(\"\\nTop 10 Most Common Last Names:\")\n",
      "print(df['Last_Name'].value_counts().head(10))\n",
      "\n",
      "# Display most common cities\n",
      "print(\"\\nTop 10 Most Common Preferred Cities:\")\n",
      "print(df['Preferred_City'].value_counts().head(10))\n",
      "print(\"\\nLeast Common Preferred Cities:\")\n",
      "print(df['Preferred_City'].value_counts().tail(5))\n",
      "\n",
      "# Display most common animals\n",
      "print(\"\\nTop 10 Most Common Preferred Animals:\")\n",
      "print(df['Preferred_Animal'].value_counts().head(10))\n",
      "print(\"\\nLeast Common Preferred Animals:\")\n",
      "print(df['Preferred_Animal'].value_counts().tail(5))\n",
      "\n",
      "# Display most common things\n",
      "print(\"\\nTop 10 Most Common Preferred Things:\")\n",
      "print(df['Preferred_Thing'].value_counts().head(10))\n",
      "print(\"\\nLeast Common Preferred Things:\")\n",
      "print(df['Preferred_Thing'].value_counts().tail(5))\n",
      "\n",
      " Generated Code: # Creating histograms and analyzing distributions\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Load the dataset\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Setting figure size\n",
      "plt.figure(figsize=(10, 6))\n",
      "\n",
      "# Distribution of preferences counts\n",
      "name_counts = df['Name'].value_counts()\n",
      "city_counts = df['Preferred_City'].value_counts()\n",
      "animal_counts = df['Preferred_Animal'].value_counts()\n",
      "thing_counts = df['Preferred_Thing'].value_counts()\n",
      "\n",
      "# Calculate statistics for distribution analysis\n",
      "print(\"=== Distribution Statistics ===\")\n",
      "print(\"\\nPreferred City Distribution:\")\n",
      "print(f\"Mean count per city: {city_counts.mean():.2f}\")\n",
      "print(f\"Median count per city: {city_counts.median():.2f}\")\n",
      "print(f\"Min count: {city_counts.min()} (City: {city_counts.idxmin()})\")\n",
      "print(f\"Max count: {city_counts.max()} (City: {city_counts.idxmax()})\")\n",
      "print(f\"Standard deviation: {city_counts.std():.2f}\")\n",
      "\n",
      "print(\"\\nPreferred Animal Distribution:\")\n",
      "print(f\"Mean count per animal: {animal_counts.mean():.2f}\")\n",
      "print(f\"Median count per animal: {animal_counts.median():.2f}\")\n",
      "print(f\"Min count: {animal_counts.min()} (Animal: {animal_counts.idxmin()})\")\n",
      "print(f\"Max count: {animal_counts.max()} (Animal: {animal_counts.idxmax()})\")\n",
      "print(f\"Standard deviation: {animal_counts.std():.2f}\")\n",
      "\n",
      "print(\"\\nPreferred Thing Distribution:\")\n",
      "print(f\"Mean count per thing: {thing_counts.mean():.2f}\")\n",
      "print(f\"Median count per thing: {thing_counts.median():.2f}\")\n",
      "print(f\"Min count: {thing_counts.min()} (Thing: {thing_counts.idxmin()})\")\n",
      "print(f\"Max count: {thing_counts.max()} (Thing: {thing_counts.idxmax()})\")\n",
      "print(f\"Standard deviation: {thing_counts.std():.2f}\")\n",
      "\n",
      "# Check for name uniqueness and distribution\n",
      "name_uniqueness = len(df['Name'].unique())\n",
      "name_dup_count = df['Name'].duplicated().sum()\n",
      "print(f\"\\nUnique names: {name_uniqueness}\")\n",
      "print(f\"Duplicated names: {name_dup_count}\")\n",
      "print(f\"Most common name appears {name_counts.max()} times: {name_counts.idxmax()}\")\n",
      "\n",
      "# Check for uniform distribution in data\n",
      "print(\"\\n=== Testing for Uniform Distribution ===\")\n",
      "# If distribution is uniform, we expect similar counts for each value\n",
      "print(\"Standard deviation as percentage of mean:\")\n",
      "print(f\"Cities: {(city_counts.std()/city_counts.mean())*100:.2f}%\")\n",
      "print(f\"Animals: {(animal_counts.std()/animal_counts.mean())*100:.2f}%\")\n",
      "print(f\"Things: {(thing_counts.std()/thing_counts.mean())*100:.2f}%\")\n",
      "\n",
      "# Examine pairwise relationships for common combinations\n",
      "print(\"\\n=== Top Combinations ===\")\n",
      "print(\"Top 5 City-Animal Combinations:\")\n",
      "print(df.groupby(['Preferred_City', 'Preferred_Animal']).size().sort_values(ascending=False).head(5))\n",
      "\n",
      "print(\"\\nTop 5 Animal-Thing Combinations:\")\n",
      "print(df.groupby(['Preferred_Animal', 'Preferred_Thing']).size().sort_values(ascending=False).head(5))\n",
      "\n",
      "# Check for interesting correlations - if people with the same first name prefer similar things\n",
      "print(\"\\n=== First Name Preference Patterns ===\")\n",
      "first_names = df['Name'].str.split().str[0]\n",
      "top_first_names = first_names.value_counts().head(5).index\n",
      "\n",
      "for name in top_first_names:\n",
      "    name_filter = df['Name'].str.startswith(name + ' ')\n",
      "    print(f\"\\nTop preferences for people with first name '{name}':\")\n",
      "    print(f\"Top cities: {df[name_filter]['Preferred_City'].value_counts().head(3).to_dict()}\")\n",
      "    print(f\"Top animals: {df[name_filter]['Preferred_Animal'].value_counts().head(3).to_dict()}\")\n",
      "    print(f\"Top things: {df[name_filter]['Preferred_Thing'].value_counts().head(3).to_dict()}\")\n",
      "\n",
      " Generated Code: # Checking for outliers in the frequency distributions\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "\n",
      "# Load the dataset\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Get value counts for each categorical column\n",
      "city_counts = df['Preferred_City'].value_counts()\n",
      "animal_counts = df['Preferred_Animal'].value_counts()\n",
      "thing_counts = df['Preferred_Thing'].value_counts()\n",
      "\n",
      "# Function to identify outliers using Z-score method\n",
      "def find_outliers_zscore(data, threshold=3):\n",
      "    z_scores = stats.zscore(data)\n",
      "    outliers = data[np.abs(z_scores) > threshold]\n",
      "    return outliers, z_scores\n",
      "\n",
      "# Apply outlier detection\n",
      "print(\"=== Outlier Detection (Z-score method) ===\")\n",
      "\n",
      "# Cities\n",
      "city_outliers, city_zscores = find_outliers_zscore(city_counts.values)\n",
      "if len(city_outliers) > 0:\n",
      "    print(\"\\nOutlier cities (by frequency):\")\n",
      "    for outlier in city_outliers:\n",
      "        city = city_counts[city_counts == outlier].index[0]\n",
      "        z = city_zscores[city_counts.values == outlier][0]\n",
      "        print(f\"{city}: {outlier} occurrences (z-score: {z:.2f})\")\n",
      "else:\n",
      "    print(\"\\nNo outlier cities detected\")\n",
      "\n",
      "# Animals\n",
      "animal_outliers, animal_zscores = find_outliers_zscore(animal_counts.values)\n",
      "if len(animal_outliers) > 0:\n",
      "    print(\"\\nOutlier animals (by frequency):\")\n",
      "    for outlier in animal_outliers:\n",
      "        animal = animal_counts[animal_counts == outlier].index[0]\n",
      "        z = animal_zscores[animal_counts.values == outlier][0]\n",
      "        print(f\"{animal}: {outlier} occurrences (z-score: {z:.2f})\")\n",
      "else:\n",
      "    print(\"\\nNo outlier animals detected\")\n",
      "\n",
      "# Things\n",
      "thing_outliers, thing_zscores = find_outliers_zscore(thing_counts.values)\n",
      "if len(thing_outliers) > 0:\n",
      "    print(\"\\nOutlier things (by frequency):\")\n",
      "    for outlier in thing_outliers:\n",
      "        thing = thing_counts[thing_counts == outlier].index[0]\n",
      "        z = thing_zscores[thing_counts.values == outlier][0]\n",
      "        print(f\"{thing}: {outlier} occurrences (z-score: {z:.2f})\")\n",
      "else:\n",
      "    print(\"\\nNo outlier things detected\")\n",
      "\n",
      "# Generate visualizations for distributions\n",
      "plt.figure(figsize=(15, 10))\n",
      "\n",
      "# City distribution\n",
      "plt.subplot(3, 1, 1)\n",
      "plt.hist(city_counts.values, bins=20)\n",
      "plt.title('Distribution of City Frequencies')\n",
      "plt.xlabel('Frequency')\n",
      "plt.ylabel('Count')\n",
      "\n",
      "# Animal distribution\n",
      "plt.subplot(3, 1, 2)\n",
      "plt.hist(animal_counts.values, bins=20)\n",
      "plt.title('Distribution of Animal Frequencies')\n",
      "plt.xlabel('Frequency')\n",
      "plt.ylabel('Count')\n",
      "\n",
      "# Thing distribution\n",
      "plt.subplot(3, 1, 3)\n",
      "plt.hist(thing_counts.values, bins=20)\n",
      "plt.title('Distribution of Thing Frequencies')\n",
      "plt.xlabel('Frequency')\n",
      "plt.ylabel('Count')\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('distributions.png')\n",
      "\n",
      "# Calculate IQR for each distribution to identify outliers using another method\n",
      "def find_outliers_iqr(data):\n",
      "    Q1 = np.percentile(data, 25)\n",
      "    Q3 = np.percentile(data, 75)\n",
      "    IQR = Q3 - Q1\n",
      "    lower_bound = Q1 - 1.5 * IQR\n",
      "    upper_bound = Q3 + 1.5 * IQR\n",
      "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
      "    return outliers, lower_bound, upper_bound, Q1, Q3, IQR\n",
      "\n",
      "print(\"\\n=== Outlier Detection (IQR method) ===\")\n",
      "\n",
      "# Cities\n",
      "city_outliers_iqr, lb_city, ub_city, Q1_city, Q3_city, IQR_city = find_outliers_iqr(city_counts.values)\n",
      "print(\"\\nCity Distribution:\")\n",
      "print(f\"Q1: {Q1_city:.2f}, Q3: {Q3_city:.2f}, IQR: {IQR_city:.2f}\")\n",
      "print(f\"Lower bound: {lb_city:.2f}, Upper bound: {ub_city:.2f}\")\n",
      "if len(city_outliers_iqr) > 0:\n",
      "    print(\"Outlier cities (IQR method):\")\n",
      "    for outlier in city_outliers_iqr:\n",
      "        city = city_counts[city_counts == outlier].index[0]\n",
      "        print(f\"{city}: {outlier} occurrences\")\n",
      "else:\n",
      "    print(\"No outlier cities detected with IQR method\")\n",
      "\n",
      "# Animals\n",
      "animal_outliers_iqr, lb_animal, ub_animal, Q1_animal, Q3_animal, IQR_animal = find_outliers_iqr(animal_counts.values)\n",
      "print(\"\\nAnimal Distribution:\")\n",
      "print(f\"Q1: {Q1_animal:.2f}, Q3: {Q3_animal:.2f}, IQR: {IQR_animal:.2f}\")\n",
      "print(f\"Lower bound: {lb_animal:.2f}, Upper bound: {ub_animal:.2f}\")\n",
      "if len(animal_outliers_iqr) > 0:\n",
      "    print(\"Outlier animals (IQR method):\")\n",
      "    for outlier in animal_outliers_iqr:\n",
      "        animal = animal_counts[animal_counts == outlier].index[0]\n",
      "        print(f\"{animal}: {outlier} occurrences\")\n",
      "else:\n",
      "    print(\"No outlier animals detected with IQR method\")\n",
      "\n",
      "# Things\n",
      "thing_outliers_iqr, lb_thing, ub_thing, Q1_thing, Q3_thing, IQR_thing = find_outliers_iqr(thing_counts.values)\n",
      "print(\"\\nThing Distribution:\")\n",
      "print(f\"Q1: {Q1_thing:.2f}, Q3: {Q3_thing:.2f}, IQR: {IQR_thing:.2f}\")\n",
      "print(f\"Lower bound: {lb_thing:.2f}, Upper bound: {ub_thing:.2f}\")\n",
      "if len(thing_outliers_iqr) > 0:\n",
      "    print(\"Outlier things (IQR method):\")\n",
      "    for outlier in thing_outliers_iqr:\n",
      "        thing = thing_counts[thing_counts == outlier].index[0]\n",
      "        print(f\"{thing}: {outlier} occurrences\")\n",
      "else:\n",
      "    print(\"No outlier things detected with IQR method\")\n",
      "\n",
      "print(\"\\nPlot saved as 'distributions.png'\")\n",
      "\n",
      "# Combine first and last names to check uniqueness\n",
      "df['First_Name'] = df['Name'].str.split().str[0]\n",
      "df['Last_Name'] = df['Name'].str.split().str[1]\n",
      "\n",
      "# Check for the total number of unique first and last names\n",
      "unique_first_names = df['First_Name'].nunique()\n",
      "unique_last_names = df['Last_Name'].nunique()\n",
      "\n",
      "print(f\"\\nUnique first names: {unique_first_names}\")\n",
      "print(f\"Unique last names: {unique_last_names}\")\n",
      "\n",
      "# Check if first and last names follow a uniform distribution\n",
      "first_name_counts = df['First_Name'].value_counts()\n",
      "last_name_counts = df['Last_Name'].value_counts()\n",
      "\n",
      "print(\"\\nFirst Name Distribution:\")\n",
      "print(f\"Mean: {first_name_counts.mean():.2f}\")\n",
      "print(f\"Median: {first_name_counts.median():.2f}\")\n",
      "print(f\"Min: {first_name_counts.min()} (Name: {first_name_counts.idxmin()})\")\n",
      "print(f\"Max: {first_name_counts.max()} (Name: {first_name_counts.idxmax()})\")\n",
      "print(f\"Std Dev: {first_name_counts.std():.2f}\")\n",
      "\n",
      "print(\"\\nLast Name Distribution:\")\n",
      "print(f\"Mean: {last_name_counts.mean():.2f}\")\n",
      "print(f\"Median: {last_name_counts.median():.2f}\")\n",
      "print(f\"Min: {last_name_counts.min()} (Name: {last_name_counts.idxmin()})\")\n",
      "print(f\"Max: {last_name_counts.max()} (Name: {last_name_counts.idxmax()})\")\n",
      "print(f\"Std Dev: {last_name_counts.std():.2f}\")\n",
      "\n",
      " Generated Code: # Analyzing patterns and conditional preferences\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Load the dataset\n",
      "df = pd.read_csv('data.csv')\n",
      "df['First_Name'] = df['Name'].str.split().str[0]\n",
      "df['Last_Name'] = df['Name'].str.split().str[1]\n",
      "\n",
      "# Check overall independence between variables\n",
      "print(\"=== Analyzing Independence Between Variables ===\")\n",
      "\n",
      "# Check for associations between first names and preferred cities\n",
      "print(\"\\nTop City for Each Common First Name:\")\n",
      "common_names = df['First_Name'].value_counts().head(10).index\n",
      "for name in common_names:\n",
      "    top_city = df[df['First_Name'] == name]['Preferred_City'].value_counts().idxmax()\n",
      "    city_count = df[(df['First_Name'] == name) & (df['Preferred_City'] == top_city)].shape[0]\n",
      "    total_name_count = df[df['First_Name'] == name].shape[0]\n",
      "    percentage = (city_count / total_name_count) * 100\n",
      "    print(f\"{name}: {top_city} ({city_count} out of {total_name_count}, {percentage:.2f}%)\")\n",
      "\n",
      "# Check for patterns in animal preferences\n",
      "print(\"\\nTop Animal for Each Common City:\")\n",
      "common_cities = df['Preferred_City'].value_counts().head(10).index\n",
      "for city in common_cities:\n",
      "    top_animal = df[df['Preferred_City'] == city]['Preferred_Animal'].value_counts().idxmax()\n",
      "    animal_count = df[(df['Preferred_City'] == city) & (df['Preferred_Animal'] == top_animal)].shape[0]\n",
      "    total_city_count = df[df['Preferred_City'] == city].shape[0]\n",
      "    percentage = (animal_count / total_city_count) * 100\n",
      "    print(f\"{city}: {top_animal} ({animal_count} out of {total_city_count}, {percentage:.2f}%)\")\n",
      "\n",
      "# Create a contingency table for Name-City pairs\n",
      "print(\"\\nContingency Analysis:\")\n",
      "# Sample a subset to keep output manageable\n",
      "name_sample = df['First_Name'].value_counts().head(5).index\n",
      "city_sample = df['Preferred_City'].value_counts().head(5).index\n",
      "\n",
      "# Filter to only these popular names and cities\n",
      "sample_df = df[df['First_Name'].isin(name_sample) & df['Preferred_City'].isin(city_sample)]\n",
      "contingency = pd.crosstab(sample_df['First_Name'], sample_df['Preferred_City'])\n",
      "print(contingency)\n",
      "\n",
      "# Calculate expected values if variables were independent\n",
      "row_totals = contingency.sum(axis=1).values.reshape(-1, 1)\n",
      "col_totals = contingency.sum(axis=0).values.reshape(1, -1)\n",
      "total = contingency.values.sum()\n",
      "expected = np.dot(row_totals, col_totals) / total\n",
      "expected_df = pd.DataFrame(expected, index=contingency.index, columns=contingency.columns)\n",
      "\n",
      "print(\"\\nExpected frequencies if independent:\")\n",
      "print(expected_df.round(2))\n",
      "\n",
      "# Display differences between observed and expected\n",
      "print(\"\\nDifference (Observed - Expected):\")\n",
      "diff = contingency - expected_df\n",
      "print(diff.round(2))\n",
      "\n",
      "# Check for distribution of preferences across genders (inferring from first names)\n",
      "# Note: This is a simplistic approach for analysis purposes only\n",
      "typical_female_names = ['Dorothy', 'Sarah', 'Elizabeth', 'Nancy', 'Jessica', 'Michelle', 'Jennifer']\n",
      "typical_male_names = ['Mark', 'Joseph', 'Charles', 'Christopher', 'David', 'Richard', 'Thomas']\n",
      "\n",
      "# Create a simple gender column\n",
      "df['Inferred_Gender'] = 'Unknown'\n",
      "df.loc[df['First_Name'].isin(typical_female_names), 'Inferred_Gender'] = 'Female'\n",
      "df.loc[df['First_Name'].isin(typical_male_names), 'Inferred_Gender'] = 'Male'\n",
      "\n",
      "# Analyze preferences by inferred gender\n",
      "print(\"\\n=== Analysis by Inferred Gender ===\")\n",
      "print(f\"Records with inferred gender: {df[df['Inferred_Gender'] != 'Unknown'].shape[0]}\")\n",
      "print(f\"Female-identified records: {df[df['Inferred_Gender'] == 'Female'].shape[0]}\")\n",
      "print(f\"Male-identified records: {df[df['Inferred_Gender'] == 'Male'].shape[0]}\")\n",
      "\n",
      "print(\"\\nTop 3 City Preferences by Gender:\")\n",
      "for gender in ['Female', 'Male']:\n",
      "    top_cities = df[df['Inferred_Gender'] == gender]['Preferred_City'].value_counts().head(3)\n",
      "    print(f\"{gender}: {dict(zip(top_cities.index, top_cities.values))}\")\n",
      "\n",
      "print(\"\\nTop 3 Animal Preferences by Gender:\")\n",
      "for gender in ['Female', 'Male']:\n",
      "    top_animals = df[df['Inferred_Gender'] == gender]['Preferred_Animal'].value_counts().head(3)\n",
      "    print(f\"{gender}: {dict(zip(top_animals.index, top_animals.values))}\")\n",
      "\n",
      "print(\"\\nTop 3 Thing Preferences by Gender:\")\n",
      "for gender in ['Female', 'Male']:\n",
      "    top_things = df[df['Inferred_Gender'] == gender]['Preferred_Thing'].value_counts().head(3)\n",
      "    print(f\"{gender}: {dict(zip(top_things.index, top_things.values))}\")\n",
      "\n",
      "# Check for name duplications and what that means for the data\n",
      "name_duplication = df['Name'].value_counts()\n",
      "print(\"\\n=== Analyzing Name Duplication ===\")\n",
      "print(f\"Number of unique full names: {len(name_duplication)}\")\n",
      "print(f\"Names appearing exactly once: {(name_duplication == 1).sum()}\")\n",
      "print(f\"Names appearing more than once: {(name_duplication > 1).sum()}\")\n",
      "print(f\"Maximum number of duplications: {name_duplication.max()} (for {name_duplication.idxmax()})\")\n",
      "\n",
      "print(\"\\nExamining duplicate name entries:\")\n",
      "most_common_name = name_duplication.idxmax()\n",
      "duplicates = df[df['Name'] == most_common_name]\n",
      "print(f\"Preferences for '{most_common_name}':\")\n",
      "print(duplicates[['Preferred_City', 'Preferred_Animal', 'Preferred_Thing']].head(5))\n"
     ]
    }
   ],
   "source": [
    "query = \"Within the file 'data.csv', how many individuals with the first name 'Kimberly' have 'Crocodile' as their favourite animal?\"\n",
    "\n",
    "# Invoke the agent asynchcronously and stream the response\n",
    "response_text = \"\"\n",
    "async for event in agent.stream_async(query):\n",
    "    if \"data\" in event:\n",
    "        # Stream text response\n",
    "        chunk = event[\"data\"]\n",
    "        response_text += chunk\n",
    "        print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6b3ce0963d4a83",
   "metadata": {},
   "source": [
    "## 8. Cleanup\n",
    "\n",
    "Finally, we'll clean up by stopping the Code Interpreter session. Once finished using a session, the session should be shopped to release resources and avoid unnecessary charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "baa2ca7fce8b181d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:31.525724Z",
     "start_time": "2025-07-13T09:35:30.947964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Interpreter session stopped successfully!\n"
     ]
    }
   ],
   "source": [
    "# Stop the Code Interpreter session\n",
    "code_client.stop()\n",
    "print(\"Code Interpreter session stopped successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a01546f53c21a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
